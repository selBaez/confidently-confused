# Confidently Confused: Exploring Expressions of Uncertainty and Confidence Calibration in Large Language Models Across Languages

## Objective
To investigate the expression of uncertainty in large language models (LLMs) across different languages, analyse their confidence levels when providing information, and understand how they respond to corrections.

## Background
Recent studies have shown that LLMs have become increasingly fluent and coherent in generating human-like text. However, there is still a considerable gap in their ability to interpret and generate expressions of uncertainty, which is crucial in supporting human decision-making. Two relevant papers on this topic have provided insights into the self-evaluation of LLMs and the impact of injecting uncertainty expressions into prompts. This project aims to build on these findings and further explore how LLMs express certainty and deal with corrections across different languages, including low-resource languages and multilingual settings. Understanding the impact of different languages on the expression of uncertainty will enable the development of more accurate and reliable LLMs for a wider range of users.

## Hackathon Research Questions
1. How do LLMs express their certainty or uncertainty in their responses across different languages?
2. Are LLMs overconfident when providing incorrect information?
3. How do LLMs respond to and incorporate corrections?
4. What is the impact of low-resource languages and multilinguality on the phenomenon of uncertainty in LLMs?

## Methodology
1. **Data collection**: Compile a dataset of diverse questions and prompts in different languages that require varying degrees of certainty in the responses. Include questions with known correct answers and those that demand expressions of uncertainty.

2. **Model evaluation**: Using a pre-trained LLM, such as GPT-3 or GPT-4, generate responses to the collected prompts in various languages. Record the model's expressions of certainty or uncertainty, as well as its confidence scores when providing information.

3. **Corrections and feedback**: Introduce corrections to the model's responses and analyze how the model incorporates the feedback in its subsequent responses across different languages. Investigate the model's behavior when dealing with corrected information and its impact on the expression of certainty.

4. **Analysis**: Evaluate the model's performance in terms of the appropriateness of its certainty expressions, its confidence calibration, and its ability to incorporate corrections across different languages. Identify patterns and potential areas for improvement in the model's uncertainty expression.

## Deliverables

1.  A presentation detailing the findings of the project, including insights into LLMs' expression of certainty, confidence calibration, and response to corrections.
2.  A set of recommendations for improving LLMs' ability to express uncertainty and incorporate corrections.
3.  A report summarizing the project's findings and potential applications in real-world scenarios.

## Timeline

The main project will be executed during a hackathon, with the research, analysis, and deliverables 1&2 completed within the event's designated time frame. If the results of the hackathon prove to be promising, a comprehensive report or short paper will be developed to build upon and share the insights gained from the event.
